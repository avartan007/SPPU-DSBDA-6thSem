{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e8b7707-15e3-4f41-9ae0-1386c2f6d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\avart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\avart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\avart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad03557-e803-445f-9c70-6888ca4e30a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Natural language processing (NLP) is a field of artificial intelligence (AI) that enables machines to understand and respond to human language.\n",
      "It involves tasks such as tokenization, POS tagging, stop word removal, stemming, and lemmatization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a sample document\n",
    "document = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence (AI) that enables machines to understand and respond to human language.\n",
    "It involves tasks such as tokenization, POS tagging, stop word removal, stemming, and lemmatization.\n",
    "\"\"\"\n",
    "print(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c27a1b-c114-4029-b383-61705d6de944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'enables', 'machines', 'to', 'understand', 'and', 'respond', 'to', 'human', 'language', '.', 'It', 'involves', 'tasks', 'such', 'as', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize into words\n",
    "tokens = word_tokenize(document)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd6c10b-9470-420c-a6f4-b66ef7795fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('that', 'IN'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('respond', 'VB'), ('to', 'TO'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('word', 'NN'), ('removal', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de73c7b-b0e2-4459-ac32-82bb7c3ce0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['Natural', 'language', 'processing', 'NLP', 'field', 'artificial', 'intelligence', 'AI', 'enables', 'machines', 'understand', 'respond', 'human', 'language', 'involves', 'tasks', 'tokenization', 'POS', 'tagging', 'stop', 'word', 'removal', 'stemming', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e848962d-80a9-4a59-99d5-d175fbbbc703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['natur', 'languag', 'process', 'nlp', 'field', 'artifici', 'intellig', 'ai', 'enabl', 'machin', 'understand', 'respond', 'human', 'languag', 'involv', 'task', 'token', 'po', 'tag', 'stop', 'word', 'remov', 'stem', 'lemmat']\n"
     ]
    }
   ],
   "source": [
    "# Apply Porter Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946d6fb7-f6d8-4c8a-a753-1afa6e17b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['Natural', 'language', 'processing', 'NLP', 'field', 'artificial', 'intelligence', 'AI', 'enables', 'machine', 'understand', 'respond', 'human', 'language', 'involves', 'task', 'tokenization', 'POS', 'tagging', 'stop', 'word', 'removal', 'stemming', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "# Apply WordNet Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a750848-f7a9-4199-b24a-7c8833fb4819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "          ai       and  artificial        as  assistants  chatbots      deep  \\\n",
      "0  0.167345  0.197673    0.127270  0.167345    0.000000  0.000000  0.000000   \n",
      "1  0.000000  0.202513    0.260772  0.000000    0.000000  0.000000  0.342884   \n",
      "2  0.000000  0.202513    0.000000  0.000000    0.342884  0.342884  0.000000   \n",
      "\n",
      "    enables   engines     field  ...      such   tagging     tasks      that  \\\n",
      "0  0.167345  0.000000  0.167345  ...  0.167345  0.167345  0.167345  0.167345   \n",
      "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.342884  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "         to  tokenization  understand    useful   virtual      word  \n",
      "0  0.334689      0.167345    0.167345  0.000000  0.000000  0.167345  \n",
      "1  0.000000      0.000000    0.000000  0.000000  0.000000  0.000000  \n",
      "2  0.000000      0.000000    0.000000  0.342884  0.342884  0.000000  \n",
      "\n",
      "[3 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create sample corpus (multiple documents)\n",
    "corpus = [\n",
    "    document,\n",
    "    \"Artificial intelligence includes machine learning and deep learning.\",\n",
    "    \"NLP is useful for chatbots, search engines, and virtual assistants.\"\n",
    "]\n",
    "\n",
    "# TF-IDF Calculation\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display TF-IDF matrix as DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d81916-32af-4d61-9050-891ae80275ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
